{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 2 3 4 5 6]\n",
      "Total Datapoints: 139850\n",
      "Counts Per Class:\n",
      "0    19950\n",
      "1    20000\n",
      "2    20000\n",
      "3    20000\n",
      "4    20000\n",
      "5    20000\n",
      "6    19900\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>explicit</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.633</td>\n",
       "      <td>106471</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-16.389</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7970</td>\n",
       "      <td>167.679</td>\n",
       "      <td>0.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.420</td>\n",
       "      <td>232933</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7860</td>\n",
       "      <td>10</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-19.388</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>123.089</td>\n",
       "      <td>0.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993</td>\n",
       "      <td>0.394</td>\n",
       "      <td>177981</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>5</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-9.779</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>74.761</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.730</td>\n",
       "      <td>0.618</td>\n",
       "      <td>125300</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>67.141</td>\n",
       "      <td>0.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993</td>\n",
       "      <td>0.475</td>\n",
       "      <td>188600</td>\n",
       "      <td>0.4070</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>9</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-13.011</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>74.130</td>\n",
       "      <td>0.594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  duration_ms  energy  explicit  \\\n",
       "0         0.782         0.633       106471  0.2610         1   \n",
       "0         0.988         0.420       232933  0.0909         0   \n",
       "0         0.993         0.394       177981  0.2580         0   \n",
       "0         0.730         0.618       125300  0.2720         1   \n",
       "0         0.993         0.475       188600  0.4070         0   \n",
       "\n",
       "   instrumentalness  key  liveness  loudness  mode  speechiness    tempo  \\\n",
       "0            0.0000    1     0.235   -16.389     1       0.7970  167.679   \n",
       "0            0.7860   10     0.104   -19.388     1       0.0409  123.089   \n",
       "0            0.0770    5     0.153    -9.779     0       0.1100   74.761   \n",
       "0            0.0000    6     0.146   -18.515     1       0.7310   67.141   \n",
       "0            0.0134    9     0.116   -13.011     1       0.0492   74.130   \n",
       "\n",
       "   valence  \n",
       "0    0.655  \n",
       "0    0.227  \n",
       "0    0.340  \n",
       "0    0.449  \n",
       "0    0.594  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import multivariate_normal # MVN not univariate\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "file_path = \"data.csv\"\n",
    "df = pd.read_csv(file_path, index_col='year')\n",
    "\n",
    "file_path = \"tracks_features.csv\"\n",
    "df2 = pd.read_csv(file_path, index_col='year')\n",
    "\n",
    "df.drop(['artists','id', 'name', 'release_date','popularity' ], axis = 1, inplace = True)\n",
    "# df2.drop(['artists','id', 'name', 'release_date', 'album_id','artist_ids', 'time_signature',\\\n",
    "#           'track_number', 'disc_number', 'album' ], axis = 1, inplace = True)\n",
    "# df = pd.concat([df,df2])\n",
    "# df.drop(['mode','explicit'], axis = 1, inplace = True)\n",
    "# print(df)\n",
    "\n",
    "### Only look at decades from 50s to 10s (2020 not included)\n",
    "l_drop = np.arange(1921,1950)\n",
    "l_drop = np.append(l_drop,2020)\n",
    "# drop_vals = np.array([0,1900,1908,1909,1917,1920])\n",
    "# l_drop = np.concatenate((l_drop,drop_vals))\n",
    "\n",
    "\n",
    "df.drop(labels=l_drop, axis=0, inplace = True)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "labels = df.index\n",
    "standardized_labels = np.array(labels)\n",
    "enc.fit(df.index.unique())\n",
    "\n",
    "\n",
    "\n",
    "lmao = df.index\n",
    "y = enc.transform(standardized_labels)\n",
    "Y = enc.transform(np.unique(standardized_labels))\n",
    "\n",
    "y_decade = y//10\n",
    "Y_decade = np.unique(y_decade)\n",
    "\n",
    "# enc.fit(df['explicit'].unique())\n",
    "# df['explicit'] = enc.transform(df['explicit'])\n",
    "\n",
    "\n",
    "df.set_index(y_decade, inplace=True)\n",
    "\n",
    "aa = df.index.value_counts().sort_index().to_numpy()\n",
    "priors = aa/len(df.index)\n",
    "check = np.sum(priors)\n",
    "class_priors = np.diag(priors)\n",
    "num_classes = len(Y_decade)\n",
    "# print(df)\n",
    "\n",
    "\n",
    "N = len(df)\n",
    "print('Labels:',Y_decade)\n",
    "print('Total Datapoints:',N)\n",
    "print(\"Counts Per Class:\")\n",
    "print(df.index.value_counts().sort_index())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cov(X, lambda_reg):\n",
    "    n = X.shape[0]\n",
    "    sigma = np.cov(X)\n",
    "    # Selecting the regularization parameter should be performed using CV and a separate data subset\n",
    "    # As I only went by training set performance (overfitting) in this problem, I settled on lambda=1/n\n",
    "    sigma += lambda_reg * np.eye(n)\n",
    "    return sigma\n",
    "\n",
    "covariance = df.std()\n",
    "mean = df.mean()\n",
    "X = (df-df.mean())/df.std()\n",
    "# print(X)\n",
    "mu = X.groupby([X.index]).mean().to_numpy()\n",
    "n = mu.shape[1]\n",
    "Sigma = np.array([regularized_cov(X[y_decade == l].T,(1/n)) for l in range(num_classes)])\n",
    "# Sigma = np.array([np.cov(X[y_decade == l].T) for l in range(num_classes)])\n",
    "# print(mu)\n",
    "# print(Sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Samples: 139850.0\n",
      "Confusion matrix:\n",
      "[[ 7134  2696   992  1148  1059   900   838]\n",
      " [11025 11410  5918  3228  3780  3213  2440]\n",
      " [ 1493  5355 11046  9441  7457  5191  3073]\n",
      " [   91   304  1463  4436  3030  1978  1218]\n",
      " [    8     9    67   213   409   132    75]\n",
      " [   18   135   339   985  1651  4752  4605]\n",
      " [  181    91   175   549  2614  3834  7651]]\n",
      "Total Mumber of Misclassified Samples: 93012.0\n",
      "Empirically Estimated Probability of Error: 0.6651\n",
      "Accuracy: 0.3349159814086522\n"
     ]
    }
   ],
   "source": [
    "C = len(priors)\n",
    "class_cond_likelihoods = np.array([multivariate_normal.pdf(X, mu[j], Sigma[j]) for j in range(C)])\n",
    "# print(np.max(class_cond_likelihoods))\n",
    "\n",
    "# Class Posterior\n",
    "# P(yj | x) = p(x | yj) * P(yj) / p(x)\n",
    "class_posteriors = class_priors.dot(class_cond_likelihoods)\n",
    "\n",
    "decisions = np.argmax(class_posteriors, axis=0)\n",
    "\n",
    "sample_class_counts = np.array([sum(y == j) for j in Y_decade])\n",
    "\n",
    "\n",
    "conf_mat = np.zeros((C, C))\n",
    "display_mat = np.zeros((C,C))\n",
    "for i in range(C): # Each decision option\n",
    "    for j in range(C): # Each class label\n",
    "        ind_ij = np.argwhere((decisions==Y_decade[i]) & (y_decade==Y_decade[j]))\n",
    "        display_mat[i, j] = len(ind_ij) # Average over class sample count\n",
    "        conf_mat[i, j] = len(ind_ij)/sample_class_counts[j]\n",
    "\n",
    "print(\"Total Number of Samples:\",np.sum(display_mat))\n",
    "print(\"Confusion matrix:\")\n",
    "print(display_mat.astype(int))\n",
    "# print(1950+(Y_decade*10))\n",
    "\n",
    "correct_class_samples = np.sum(np.diag(display_mat))\n",
    "print(\"Total Mumber of Misclassified Samples: {}\".format(N - correct_class_samples))\n",
    "\n",
    "prob_error = 1 - (correct_class_samples / N)\n",
    "print(\"Empirically Estimated Probability of Error: {:.4f}\".format(prob_error))\n",
    "print(\"Accuracy:\",1-prob_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "acousticness        0.125656\n",
      "loudness            0.118565\n",
      "duration_ms         0.102303\n",
      "energy              0.096222\n",
      "valence             0.087639\n",
      "danceability        0.086591\n",
      "speechiness         0.084844\n",
      "tempo               0.080085\n",
      "liveness            0.075495\n",
      "instrumentalness    0.067957\n",
      "key                 0.047926\n",
      "explicit            0.014459\n",
      "mode                0.012258\n",
      "dtype: float64\n",
      "\n",
      "ACCURACY OF THE MODEL:  0.47906089858181383\n",
      "\n",
      "Time to run (sec):  22.790042877197266\n",
      "Time to run (min):  0.37983404795328773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y_decade, test_size = 0.30)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "print('done')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('done')\n",
    "\n",
    "feature_imp = pd.Series(clf.feature_importances_, index = df.columns).sort_values(ascending = False)\n",
    "print('done')\n",
    "\n",
    "print(feature_imp)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    " \n",
    "# metrics are used to find accuracy or error\n",
    "from sklearn import metrics \n",
    "print()\n",
    " \n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "stop = time.time()\n",
    "print()\n",
    "print('Time to run (sec): ', stop - start) \n",
    "print('Time to run (min): ', (stop - start)/60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x1b852f194c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Utility to visualize PyTorch network and shapes\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import KFold # Important new include\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP(nn.Module):\n",
    "    # Two-layer MLP (not really a perceptron activation function...) network class\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, C):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        # Fully connected layer WX + b mapping from input_dim (n) -> hidden_layer_dim\n",
    "        self.input_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        #Hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer again fully connected mapping from hidden_layer_dim -> outputs_dim (C)\n",
    "        self.output_fc = nn.Linear(hidden_dim, C)\n",
    "        # Log Softmax (faster and better than straight Softmax)\n",
    "        # dim=1 refers to the dimension along which the softmax operation is computed\n",
    "        # In this case computing probabilities across dim 1, i.e., along classes at output layer\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1) \n",
    "        \n",
    "    # Don't call this function directly!! \n",
    "    # Simply pass input to model and forward(input) returns output, e.g. model(X)\n",
    "    def forward(self, X):\n",
    "        # X = [batch_size, input_dim (n)]\n",
    "        X = self.input_fc(X)\n",
    "        \n",
    "        # Non-linear activation function, e.g. ReLU (default good choice)\n",
    "        # Could also choose F.softplus(x) for smooth-ReLU, empirically worse than ReLU\n",
    "        X = F.relu(X)\n",
    "        X = self.hidden_layer(X)\n",
    "        # X = [batch_size, hidden_dim]\n",
    "        \n",
    "        # Add another hidden layer\n",
    "        X = F.relu(X)\n",
    "        X = self.hidden_layer(X)\n",
    "\n",
    "\n",
    "        # Connect to last layer and output 'logits'\n",
    "        X = self.output_fc(X)\n",
    "        # Squash logits to probabilities that sum up to 1\n",
    "        y = self.log_softmax(X)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, data, labels, criterion, optimizer, num_epochs=25):\n",
    "\n",
    "    # Apparently good practice to set this \"flag\" too before training\n",
    "    # Does things like make sure Dropout layers are active, gradients are updated, etc.\n",
    "    # Probably not a big deal for our toy network, but still worth developing good practice\n",
    "#     model = model.to(device)\n",
    "#     data = data.to(device)\n",
    "#     print(model.device())\n",
    "    \n",
    "    model.train()\n",
    "    # Optimize the neural network\n",
    "    for epoch in range(num_epochs):\n",
    "        # These outputs represent the model's predicted probabilities for each class. \n",
    "        outputs = model(data)\n",
    "        # Criterion computes the cross entropy loss between input and target\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Set gradient buffers to zero explicitly before backprop\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass to compute the gradients through the network\n",
    "        loss.backward()\n",
    "        # GD step update\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def model_predict(model, data, labels):\n",
    "    # Similar idea to model.train(), set a flag to let network know your in \"inference\" mode\n",
    "#     model.to(device)\n",
    "    labels.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    N = len(data)\n",
    "#     print(N)\n",
    "    # Disabling gradient calculation is useful for inference, only forward pass!!\n",
    "    with torch.no_grad():\n",
    "        # Evaluate nn on test data and compare to true labels\n",
    "        predicted_labels = model(data)\n",
    "        # Back to numpy\n",
    "        predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "        Z = np.argmax(predicted_labels, 1)\n",
    "#     print(labels)\n",
    "    conf_mat = confusion_matrix(Z, labels.cpu())\n",
    "    correct_class_samples = np.sum(np.diag(conf_mat))\n",
    "    prob_error = 1 - (correct_class_samples / N)\n",
    "#     print(conf_mat)\n",
    "#     print(\"Total Number of Misclassified Samples: {:d}\".format(N - correct_class_samples))\n",
    "#     print(\"Empirically Estimated Probability of Error: {:.4f}\".format(prob_error))\n",
    "#     print()\n",
    "    return Z, prob_error, conf_mat\n",
    "\n",
    "# Z, acc = model_predict(model,X_tensor[0],labels[0])\n",
    "# print(acc)\n",
    "\n",
    "def neuron_cross_validation(X,labels,X_tensor,l_tensor):#,Xtest,labelsTest):\n",
    "\n",
    "    K = 10\n",
    "    kf = KFold(n_splits=K, shuffle=True)\n",
    "    start = 1\n",
    "    trials = 64\n",
    "    neurons = np.arange(start, start+trials, 1)\n",
    "    n_hidden = len(neurons)\n",
    "    \n",
    "#     X = X.to(device)\n",
    "#     labels = labels.to(device)\n",
    "    X_tensor = X_tensor.to(device)\n",
    "    l_tensor = l_tensor.to(device)\n",
    "    \n",
    "    # Store predictions per degree using ordered X samples for plotting best fit lines\n",
    "#     y_train_preds_ordered = np.empty(n_degs, dtype=np.ndarray)\n",
    "    # Allocate space for CV\n",
    "    # No need for training loss storage too but useful comparison\n",
    "    mse_valid_mk = np.empty((n_hidden, K)) \n",
    "    mse_train_mk = np.empty((n_hidden, K)) # Indexed by model m, data partition k\n",
    "    acc = np.empty((n_hidden, K))\n",
    "    accTest = np.empty(n_hidden)\n",
    "    i = 0\n",
    "    for n in neurons: \n",
    "        start_it = time.time()\n",
    "        k = 0\n",
    "        for train_indices, valid_indices in kf.split(X):\n",
    "            # Extract the training and validation sets from the K-fold split\n",
    "            X_train_k = X_tensor[train_indices]\n",
    "            y_train_k = l_tensor[train_indices]\n",
    "            X_train_tensor = (X_train_k)\n",
    "            y_train_tensor = (y_train_k)\n",
    "            \n",
    "            X_valid_k = X_tensor[valid_indices]\n",
    "            y_valid_k = l_tensor[valid_indices]\n",
    "            X_valid_tensor = (X_valid_k)\n",
    "\n",
    "            #Assign data to GPU\n",
    "#             X_train_k.to(device)\n",
    "#             y_train_k = labels[train_indices]\n",
    "#             X_train_tensor = torch.FloatTensor(X_train_k)\n",
    "#             y_train_tensor = torch.LongTensor(y_train_k)\n",
    "            \n",
    "#             X_valid_k = X[valid_indices]\n",
    "#             y_valid_k = labels[valid_indices]\n",
    "#             X_valid_tensor = torch.FloatTensor(X_valid_k)\n",
    "            \n",
    "            #Train it\n",
    "            model = TwoLayerMLP(input_dim, n, output_dim)\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            num_epochs = 100\n",
    "            model = model_train(model, X_train_tensor, y_train_tensor, criterion, optimizer, num_epochs=num_epochs)\n",
    "            \n",
    "            #Validate/predict it and record p(error)\n",
    "            Z, acc[i,k], conf_mat = model_predict(model,X_valid_tensor,y_valid_k)\n",
    "#             Z, accTest[i,k] = model_predict(model,X_tensor_test,labelsTest)\n",
    "            k += 1\n",
    "        print(X_train_tensor.device)\n",
    "        i+=1\n",
    "#         print(i, 'done')\n",
    "\n",
    "        stop_it = time.time()\n",
    "        t = stop_it - start_it\n",
    "#         print('Time to run {}th iteration: {}'.format(i,t)) \n",
    "        print('Time to run {}th iteration:'.format(i))\n",
    "        print('{} minutes'.format(t/60)) \n",
    "        print()\n",
    "    accuracy = np.mean(acc, axis=1)\n",
    "    min_acc = np.min(accuracy)\n",
    "    min_ind = np.argmin(accuracy)\n",
    "    n_optimal = neurons[min_ind]\n",
    "    \n",
    "    return min_ind, min_acc, n_optimal, accuracy, acc#, models, accTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "TwoLayerMLP(\n",
      "  (input_fc): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (hidden_layer): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (output_fc): Linear(in_features=16, out_features=7, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 16, 16]             224\n",
      "            Linear-2               [-1, 16, 16]             272\n",
      "            Linear-3               [-1, 16, 16]             272\n",
      "            Linear-4                [-1, 16, 7]             119\n",
      "        LogSoftmax-5                [-1, 16, 7]               0\n",
      "================================================================\n",
      "Total params: 887\n",
      "Trainable params: 887\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n",
      "done\n",
      "97895\n",
      "41955\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.device(device)\n",
    "\n",
    "# return\n",
    "input_dim = X.shape[1]\n",
    "n_hidden_neurons = 16   #VARY THIS FOR CV\n",
    "output_dim = C\n",
    "\n",
    "# It's called an MLP but really it's not...\n",
    "model = TwoLayerMLP(input_dim, n_hidden_neurons, output_dim)\n",
    "model.to(device)\n",
    "# Visualize network architecture\n",
    "print(model)\n",
    "summary(model, input_size=(n_hidden_neurons, input_dim))\n",
    "\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# X_arr = X.to_numpy()\n",
    "\n",
    "# X_tensor = torch.FloatTensor(X_arr)\n",
    "# l_tensor = torch.LongTensor(y_decade)\n",
    "\n",
    "\n",
    "# X_arr_train = X_train.to_numpy()\n",
    "# X_arr_test = X_test.to_numpy()\n",
    "\n",
    "X_tensor_train = torch.FloatTensor(X_train)\n",
    "X_tensor_test = torch.FloatTensor(X_test)\n",
    "\n",
    "\n",
    "l_tensor_train = torch.LongTensor(y_train)\n",
    "l_tensor_test = torch.LongTensor(y_test)\n",
    "\n",
    "\n",
    "# X_tensor_train = torch.FloatTensor(X_train)\n",
    "# X_tensor_test = torch.FloatTensor(X_test)\n",
    "\n",
    "\n",
    "# l_tensor_train = torch.LongTensor(y_train)\n",
    "# l_tensor_test = torch.LongTensor(y_test)\n",
    "#     X.to(device)\n",
    "#     labels.to(device)\n",
    "#     X_tensor.to(device)\n",
    "#     l_tensor.to(device)\n",
    "\n",
    "print('done')\n",
    "# Convert numpy structures to PyTorch tensors, as these are the data types required by the library\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Time to run 1th iteration:\n",
      "0.05538492600123088 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 2th iteration:\n",
      "0.060797993342081705 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 3th iteration:\n",
      "0.0602169394493103 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 4th iteration:\n",
      "0.060903310775756836 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 5th iteration:\n",
      "0.0618834654490153 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 6th iteration:\n",
      "0.06147058010101318 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 7th iteration:\n",
      "0.062339226404825844 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 8th iteration:\n",
      "0.06322081089019775 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 9th iteration:\n",
      "0.06495742400487264 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 10th iteration:\n",
      "0.06583587725957235 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 11th iteration:\n",
      "0.06698288917541503 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 12th iteration:\n",
      "0.06766855319341024 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 13th iteration:\n",
      "0.06941054264704387 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 14th iteration:\n",
      "0.07053103844324747 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 15th iteration:\n",
      "0.07193525234858195 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 16th iteration:\n",
      "0.07153893709182739 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 17th iteration:\n",
      "0.07604399919509888 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 18th iteration:\n",
      "0.07822460333506266 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 19th iteration:\n",
      "0.08199076255162557 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 20th iteration:\n",
      "0.08008954922358195 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 21th iteration:\n",
      "0.07997835477193196 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 22th iteration:\n",
      "0.08100055456161499 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 23th iteration:\n",
      "0.08206530014673868 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 24th iteration:\n",
      "0.08450406789779663 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 25th iteration:\n",
      "0.08276772896448771 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 26th iteration:\n",
      "0.0837080160776774 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 27th iteration:\n",
      "0.07839399178822835 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 28th iteration:\n",
      "0.07919493913650513 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 29th iteration:\n",
      "0.08016181389490763 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 30th iteration:\n",
      "0.08073948621749878 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 31th iteration:\n",
      "0.08177849054336547 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 32th iteration:\n",
      "0.08591390053431193 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 33th iteration:\n",
      "0.10793381532033285 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 34th iteration:\n",
      "0.10788769721984863 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 35th iteration:\n",
      "0.10695597330729166 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 36th iteration:\n",
      "0.10760912497838339 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 37th iteration:\n",
      "0.10925103028615316 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 38th iteration:\n",
      "0.10954544146855673 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 39th iteration:\n",
      "0.11101472775141398 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 40th iteration:\n",
      "0.11063950856526693 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 41th iteration:\n",
      "0.11417807340621948 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 42th iteration:\n",
      "0.11474566459655762 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 43th iteration:\n",
      "0.1148359497388204 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 44th iteration:\n",
      "0.11574572722117106 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 45th iteration:\n",
      "0.11573832035064698 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 46th iteration:\n",
      "0.1146160880724589 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 47th iteration:\n",
      "0.11618787844975789 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 48th iteration:\n",
      "0.11329187154769897 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 49th iteration:\n",
      "0.11908799409866333 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 50th iteration:\n",
      "0.11979438463846842 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 51th iteration:\n",
      "0.12261200745900472 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 52th iteration:\n",
      "0.12035257816314697 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 53th iteration:\n",
      "0.12223950624465943 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 54th iteration:\n",
      "0.12148252328236898 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 55th iteration:\n",
      "0.12437698046366373 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 56th iteration:\n",
      "0.11923019091288249 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 57th iteration:\n",
      "0.12320384979248047 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 58th iteration:\n",
      "0.12357618808746337 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 59th iteration:\n",
      "0.12608924706776936 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 60th iteration:\n",
      "0.12637130419413248 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 61th iteration:\n",
      "0.12697973648707073 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 62th iteration:\n",
      "0.12716811498006184 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 63th iteration:\n",
      "0.12771279017130535 minutes\n",
      "\n",
      "cuda:0\n",
      "Time to run 64th iteration:\n",
      "0.1258938988049825 minutes\n",
      "\n",
      "end\n",
      "\n",
      "Best no. of neurons:             64\n",
      "Probability of error (Training): 0.7085345655433842\n",
      "Accuracy (Training)            : 0.29146543445661577\n",
      "Total Time (sec):  365.89579153060913\n",
      "Total Time (min):  6.098263192176819\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# start = 5\n",
    "# neurons_test = np.arange(start, start+15, 1)\n",
    "# neurons = np.zeros(len(Ntrain))\n",
    "# prob_error = np.zeros(len(Ntrain))\n",
    "\n",
    "# X_train.to(device)\n",
    "# y_train.to(device)\n",
    "# X_tensor_train.to(device)\n",
    "# l_tensor_train.to(device)\n",
    "min_ind, min_acc, nNeurons,acc,acc_all = neuron_cross_validation(X_train,y_train,X_tensor_train,l_tensor_train)\n",
    "# neurons = nNeurons\n",
    "prob_error = min_acc\n",
    "# print(Ntrain,\"Samples...\")\n",
    "\n",
    "\n",
    "stop = time.time()\n",
    "print('end')\n",
    "print()\n",
    "print(\"Best no. of neurons:            \",nNeurons)\n",
    "print(\"Probability of error (Training):\",min_acc)\n",
    "print(\"Accuracy (Training)            :\",1-min_acc)\n",
    "print('Total Time (sec): ', stop - start) \n",
    "print('Total Time (min): ', (stop - start)/60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Best no. of neurons:             64\n",
      "Probability of error (Training): 0.7085345655433842\n",
      "Accuracy (Training): 0.29146543445661577\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"Best no. of neurons:            \",nNeurons)\n",
    "print(\"Probability of error (Training):\",min_acc)\n",
    "print(\"Accuracy (Training):\",1-min_acc)\n",
    "# print(acc)\n",
    "\n",
    "\n",
    "# plt.scatter(neurons_test,acc)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "Probability of error with 64 Neurons (Test): 0.7038970325348588\n",
      "Time (min):  0.014240769545237224\n",
      "Accuracy (Test): 0.2961029674651412\n",
      "Worst Value    : 0.14285714285714285\n",
      "Percent Per Class:\n",
      "0    14.265284\n",
      "1    14.301037\n",
      "2    14.301037\n",
      "3    14.301037\n",
      "4    14.301037\n",
      "5    14.301037\n",
      "6    14.229532\n",
      "dtype: float64\n",
      "\n",
      "Training\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# nNeurons = 64\n",
    "model = TwoLayerMLP(input_dim, nNeurons, output_dim)\n",
    "# model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "model.to(device)\n",
    "X_tensor_train = X_tensor_train.to(device) \n",
    "l_tensor_train = l_tensor_train.to(device)\n",
    "model = model_train(model, X_tensor_train, l_tensor_train, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "#Validate/predict it and record p(error)\n",
    "# y_test_tensor = Tensor\n",
    "\n",
    "X_tensor_test = X_tensor_test.to(device) \n",
    "l_tensor_test = l_tensor_test.to(device)\n",
    "Z, accTest, conf_matrix = model_predict(model,X_tensor_test,l_tensor_test)\n",
    "\n",
    "stop = time.time()\n",
    "print('Test Set')\n",
    "print('Probability of error with', nNeurons,'Neurons (Test):',accTest)\n",
    "# print('Time: ', stop - start) \n",
    "print('Time (min): ', (stop - start)/60) \n",
    "print('Accuracy (Test):',1-accTest)\n",
    "print('Worst Value    :',1/7)\n",
    "print(\"Percent Per Class:\")\n",
    "print(df.index.value_counts().sort_index()/N*100)\n",
    "print()\n",
    "print('Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 1\n",
    "trials = 64\n",
    "x_ax = np.arange(start, start+trials, 1)\n",
    "plt.plot(x_ax,acc)\n",
    "plt.title('Probability of Error vs. Neurons')\n",
    "plt.ylabel('p(Error)')\n",
    "plt.xlabel('Neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5042 3325 1820 1215 1231  829  824]\n",
      " [ 260  469  286  164  142  115   59]\n",
      " [ 367  799  982  630  495  296  147]\n",
      " [  58  487  944 1017  685  372  236]\n",
      " [  60   96  122  130  151  103   71]\n",
      " [  64  131  171  163  173  120   85]\n",
      " [ 130  594 1696 2612 3202 4143 4642]]\n"
     ]
    }
   ],
   "source": [
    "#Precision and recall of model\n",
    "print(conf_matrix)\n",
    "for i in range(len(conf_matrix)):\n",
    "    recall = np.mean(conf_matrix, axis = 0)\n",
    "    precision = np.mean(conf_matrix, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[854.42857143 843.         860.14285714 847.28571429 868.42857143\n",
      " 854.         866.28571429]\n",
      "[2040.85714286  213.57142857  530.85714286  542.71428571  104.71428571\n",
      "  129.57142857 2431.28571429]\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 64, 64]             896\n",
      "            Linear-2               [-1, 64, 64]           4,160\n",
      "            Linear-3               [-1, 64, 64]           4,160\n",
      "            Linear-4                [-1, 64, 7]             455\n",
      "        LogSoftmax-5                [-1, 64, 7]               0\n",
      "================================================================\n",
      "Total params: 9,671\n",
      "Trainable params: 9,671\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summary(model,input_size=16)\n",
    "print(recall)\n",
    "print(precision)\n",
    "summary(model, input_size=(nNeurons, input_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "# Probability of error with 400 Neurons (Test): 0.6327493743296388\n",
    "# Time (min):  5.017530063788096\n",
    "# Accuracy (Test): 0.36725062567036115\n",
    "# Worst Value    : 0.14285714285714285\n",
    "# Percent Per Class:\n",
    "# 0    14.265284\n",
    "# 1    14.301037\n",
    "# 2    14.301037\n",
    "# 3    14.301037\n",
    "# 4    14.301037\n",
    "# 5    14.301037\n",
    "# 6    14.229532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "model = TwoLayerMLP(input_dim, nNeurons, output_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "model = model_train(model, X_tensor_train, l_tensor_train, criterion, optimizer, num_epochs=num_epochs)\n",
    "#Validate/predict it and record p(error)\n",
    "Z, accTest = model_predict(model,X_tensor_test,y_test)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print('Probability of error with', nNeurons,'Neurons (Test):',accTest)\n",
    "print('Time: ', stop - start) \n",
    "print('Time (min): ', (stop - start)/60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Accuracy (Test):',1-accTest)\n",
    "print('Worst Value    :',1/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

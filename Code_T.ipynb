{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 6 6 6]\n",
      "[19950 20000 20000 20000 20000 20000 19900]\n",
      "1    20000\n",
      "2    20000\n",
      "3    20000\n",
      "4    20000\n",
      "5    20000\n",
      "0    19950\n",
      "6    19900\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import multivariate_normal # MVN not univariate\n",
    "\n",
    "file_path = \"data.csv\"\n",
    "df = pd.read_csv(file_path, index_col='year')\n",
    "\n",
    "df.drop(['artists','id', 'name', 'release_date', 'popularity' ], axis = 1, inplace = True)\n",
    "\n",
    "### Only look at decades from 50s to 10s (2020 not included)\n",
    "l_drop = np.arange(1921,1950)\n",
    "l_drop = np.append(l_drop,2020)\n",
    "df.drop(labels=l_drop, axis=0, inplace = True)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "labels = df.index\n",
    "standardized_labels = np.array(labels)\n",
    "enc.fit(df.index.unique())\n",
    "\n",
    "\n",
    "\n",
    "lmao = df.index\n",
    "y = enc.transform(standardized_labels)\n",
    "Y = enc.transform(np.unique(standardized_labels))\n",
    "\n",
    "y_decade = y//10\n",
    "Y_decade = np.unique(y_decade)\n",
    "\n",
    "enc.fit(df['explicit'].unique())\n",
    "df['explicit'] = enc.transform(df['explicit'])\n",
    "\n",
    "df.set_index(y_decade, inplace=True)\n",
    "aa = df.index.value_counts().sort_index().to_numpy()\n",
    "priors = aa/len(df.index)\n",
    "check = np.sum(priors)\n",
    "class_priors = np.diag(priors)\n",
    "num_classes = len(Y_decade)\n",
    "N = len(df)\n",
    "print(y_decade)\n",
    "print(aa)\n",
    "print(df.index.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in range(num_classes):\n",
    "#     print(X[y_decade == l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.18310513e+00 -3.76180175e-01 -1.33658892e-01 -9.41176488e-01\n",
      "  -2.70298539e-01  4.24471697e-01 -4.70573783e-02  3.02265961e-02\n",
      "  -7.66491659e-01  3.68776328e-03  1.71383679e-01 -2.58376021e-01\n",
      "  -2.18169919e-01]\n",
      " [ 5.65145585e-01 -2.63535735e-01 -2.27088451e-01 -4.48153314e-01\n",
      "  -2.99322830e-01  1.04392675e-01 -2.57446803e-02  2.86152410e-02\n",
      "  -3.84891301e-01  9.40046411e-02 -1.86726689e-01 -9.80689451e-02\n",
      "   8.80361734e-02]\n",
      " [-8.09641263e-02 -8.94161788e-02  1.64464576e-01  2.18387635e-02\n",
      "  -2.89915282e-01 -4.37424021e-02 -2.31879553e-02  9.32223570e-02\n",
      "  -1.34354301e-01  6.90618842e-02 -1.69335844e-01  5.50425254e-02\n",
      "   2.06296778e-01]\n",
      " [-3.80376582e-01  4.13842440e-02  1.41156736e-01  2.66625399e-01\n",
      "  -2.13388494e-01 -2.65793433e-02  2.94521701e-02  5.84384594e-03\n",
      "  -8.62619839e-02 -2.48077289e-04 -1.48046678e-01  9.98656911e-02\n",
      "   1.38279104e-01]\n",
      " [-3.63352415e-01  1.68693391e-01  1.15084469e-01  2.36338582e-01\n",
      "   1.27816051e-01 -8.53727851e-02  3.95228255e-02 -2.78978774e-02\n",
      "   1.50930616e-01  1.87349058e-02  3.85928927e-02  2.89321784e-02\n",
      "   6.13934050e-02]\n",
      " [-4.58178801e-01  1.94259402e-01  3.93278961e-02  4.81412610e-01\n",
      "   2.14835873e-01 -1.74298317e-01  2.63556921e-02 -4.66527139e-02\n",
      "   6.12769391e-01 -5.12372528e-02  8.52873654e-02  9.56430759e-02\n",
      "  -1.64352448e-03]\n",
      " [-4.64744749e-01  3.25482012e-01 -1.00121087e-01  3.82674881e-01\n",
      "   7.33263793e-01 -1.98804368e-01  5.44404821e-04 -8.37003840e-02\n",
      "   6.09430159e-01 -1.34667985e-01  2.10325359e-01  7.66990502e-02\n",
      "  -2.76118031e-01]]\n",
      "[[[ 3.57683395e-01 -1.33329936e-01  1.69767160e-02 ... -1.15372927e-01\n",
      "   -7.90126767e-02 -1.65505922e-01]\n",
      "  [-1.33329936e-01  9.97800252e-01 -2.21322430e-01 ...  4.66816729e-01\n",
      "    1.09051787e-01  6.30580149e-01]\n",
      "  [ 1.69767160e-02 -2.21322430e-01  1.74575121e+00 ... -5.81311863e-02\n",
      "   -5.51502712e-02 -2.94675615e-01]\n",
      "  ...\n",
      "  [-1.15372927e-01  4.66816729e-01 -5.81311863e-02 ...  2.57839535e+00\n",
      "    6.29635977e-03  1.12782475e-01]\n",
      "  [-7.90126767e-02  1.09051787e-01 -5.51502712e-02 ...  6.29635977e-03\n",
      "    1.13564215e+00  3.07993545e-01]\n",
      "  [-1.65505922e-01  6.30580149e-01 -2.94675615e-01 ...  1.12782475e-01\n",
      "    3.07993545e-01  1.17196404e+00]]\n",
      "\n",
      " [[ 7.48209585e-01 -1.21802302e-01  3.21265895e-02 ...  2.71321484e-02\n",
      "   -1.27507840e-01 -2.81437488e-01]\n",
      "  [-1.21802302e-01  8.67996752e-01 -1.90392950e-01 ...  6.69549858e-02\n",
      "   -7.80000107e-03  5.27173583e-01]\n",
      "  [ 3.21265895e-02 -1.90392950e-01  1.33219901e+00 ...  3.44481960e-02\n",
      "   -4.65739843e-02 -3.10004339e-01]\n",
      "  ...\n",
      "  [ 2.71321484e-02  6.69549858e-02  3.44481960e-02 ...  7.33157353e-01\n",
      "    1.43306306e-02  3.48911997e-02]\n",
      "  [-1.27507840e-01 -7.80000107e-03 -4.65739843e-02 ...  1.43306306e-02\n",
      "    1.03107222e+00  2.14704968e-01]\n",
      "  [-2.81437488e-01  5.27173583e-01 -3.10004339e-01 ...  3.48911997e-02\n",
      "    2.14704968e-01  1.05794089e+00]]\n",
      "\n",
      " [[ 8.19323255e-01 -6.12535952e-02 -1.05665217e-01 ... -1.33609107e-03\n",
      "   -1.39631269e-01 -2.41863136e-01]\n",
      "  [-6.12535952e-02  8.70240084e-01 -1.57637917e-01 ...  4.36259209e-02\n",
      "   -1.30208628e-01  4.91672190e-01]\n",
      "  [-1.05665217e-01 -1.57637917e-01  1.35820910e+00 ... -5.81696840e-03\n",
      "   -2.46115562e-02 -1.93128440e-01]\n",
      "  ...\n",
      "  [-1.33609107e-03  4.36259209e-02 -5.81696840e-03 ...  5.33556744e-01\n",
      "    2.94217014e-02  1.36716117e-02]\n",
      "  [-1.39631269e-01 -1.30208628e-01 -2.46115562e-02 ...  2.94217014e-02\n",
      "    9.68525986e-01  1.09368865e-01]\n",
      "  [-2.41863136e-01  4.91672190e-01 -1.93128440e-01 ...  1.36716117e-02\n",
      "    1.09368865e-01  9.92625568e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 8.32709334e-01 -1.73460412e-01 -6.84007302e-02 ... -1.08180477e-01\n",
      "   -8.52762417e-02 -1.67160154e-01]\n",
      "  [-1.73460412e-01  1.14717585e+00 -6.30277937e-02 ...  2.49046041e-01\n",
      "   -1.71979953e-01  5.74495480e-01]\n",
      "  [-6.84007302e-02 -6.30277937e-02  7.08150602e-01 ... -1.96997346e-02\n",
      "   -4.18021243e-02 -1.74627062e-01]\n",
      "  ...\n",
      "  [-1.08180477e-01  2.49046041e-01 -1.96997346e-02 ...  9.30886630e-01\n",
      "   -8.83454452e-03  7.82531815e-02]\n",
      "  [-8.52762417e-02 -1.71979953e-01 -4.18021243e-02 ... -8.83454452e-03\n",
      "    1.11256535e+00  1.17458503e-01]\n",
      "  [-1.67160154e-01  5.74495480e-01 -1.74627062e-01 ...  7.82531815e-02\n",
      "    1.17458503e-01  1.06454900e+00]]\n",
      "\n",
      " [[ 7.75428529e-01 -1.36564153e-01 -1.18468848e-02 ... -9.92564150e-02\n",
      "   -1.26135552e-01 -1.57125554e-01]\n",
      "  [-1.36564153e-01  1.09472758e+00 -8.68817887e-02 ...  1.71004890e-01\n",
      "   -1.83961040e-01  5.71950828e-01]\n",
      "  [-1.18468848e-02 -8.68817887e-02  7.59303990e-01 ...  1.20416119e-02\n",
      "   -3.45830330e-02 -1.60034545e-01]\n",
      "  ...\n",
      "  [-9.92564150e-02  1.71004890e-01  1.20416119e-02 ...  9.19384827e-01\n",
      "   -9.31614300e-03  1.04906986e-01]\n",
      "  [-1.26135552e-01 -1.83961040e-01 -3.45830330e-02 ... -9.31614300e-03\n",
      "    1.12578379e+00  5.45853464e-02]\n",
      "  [-1.57125554e-01  5.71950828e-01 -1.60034545e-01 ...  1.04906986e-01\n",
      "    5.45853464e-02  1.02511269e+00]]\n",
      "\n",
      " [[ 7.44873206e-01 -1.50227391e-01 -1.92570392e-02 ... -5.87404805e-02\n",
      "   -1.38659609e-01 -1.46597679e-01]\n",
      "  [-1.50227391e-01  1.03258121e+00 -1.18582638e-01 ...  2.15065905e-01\n",
      "    9.91636037e-04  4.01603976e-01]\n",
      "  [-1.92570392e-02 -1.18582638e-01  6.64052231e-01 ... -3.16061694e-02\n",
      "   -1.51837304e-02 -1.05402168e-01]\n",
      "  ...\n",
      "  [-5.87404805e-02  2.15065905e-01 -3.16061694e-02 ...  1.10677542e+00\n",
      "    7.16947423e-02  4.59801853e-02]\n",
      "  [-1.38659609e-01  9.91636037e-04 -1.51837304e-02 ...  7.16947423e-02\n",
      "    1.09698229e+00  9.35317023e-02]\n",
      "  [-1.46597679e-01  4.01603976e-01 -1.05402168e-01 ...  4.59801853e-02\n",
      "    9.35317023e-02  9.24279602e-01]]]\n"
     ]
    }
   ],
   "source": [
    "def regularized_cov(X, lambda_reg):\n",
    "    n = X.shape[0]\n",
    "    sigma = np.cov(X)\n",
    "    # Selecting the regularization parameter should be performed using CV and a separate data subset\n",
    "    # As I only went by training set performance (overfitting) in this problem, I settled on lambda=1/n\n",
    "    sigma += lambda_reg * np.eye(n)\n",
    "    return sigma\n",
    "\n",
    "covariance = df.std()\n",
    "mean = df.mean()\n",
    "X = (df-df.mean())/df.std()\n",
    "\n",
    "mu = X.groupby([X.index]).mean().to_numpy()\n",
    "n = mu.shape[1]\n",
    "Sigma = np.array([regularized_cov(X[y_decade == l].T,(1/n)) for l in range(num_classes)])\n",
    "# Sigma = np.array([np.cov(X[y_decade == l].T) for l in range(num_classes)])\n",
    "print(mu)\n",
    "print(Sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.056959708131163e-05\n",
      "Confusion matrix:\n",
      "[[ 7134  2696   992  1148  1059   900   838]\n",
      " [11025 11410  5918  3228  3780  3213  2440]\n",
      " [ 1493  5355 11046  9441  7457  5191  3073]\n",
      " [   91   304  1463  4436  3030  1978  1218]\n",
      " [    8     9    67   213   409   132    75]\n",
      " [   18   135   339   985  1651  4752  4605]\n",
      " [  181    91   175   549  2614  3834  7651]]\n",
      "139850.0\n",
      "Total Mumber of Misclassified Samples: 93012.0\n",
      "Empirically Estimated Probability of Error: 0.6651\n"
     ]
    }
   ],
   "source": [
    "C = len(priors)\n",
    "class_cond_likelihoods = np.array([multivariate_normal.pdf(X, mu[j], Sigma[j]) for j in range(C)])\n",
    "print(np.max(class_cond_likelihoods))\n",
    "\n",
    "# Class Posterior\n",
    "# P(yj | x) = p(x | yj) * P(yj) / p(x)\n",
    "class_posteriors = class_priors.dot(class_cond_likelihoods)\n",
    "\n",
    "decisions = np.argmax(class_posteriors, axis=0)\n",
    "\n",
    "sample_class_counts = np.array([sum(y == j) for j in Y_decade])\n",
    "\n",
    "\n",
    "conf_mat = np.zeros((C, C))\n",
    "display_mat = np.zeros((C,C))\n",
    "for i in range(C): # Each decision option\n",
    "    for j in range(C): # Each class label\n",
    "        ind_ij = np.argwhere((decisions==Y_decade[i]) & (y_decade==Y_decade[j]))\n",
    "        display_mat[i, j] = len(ind_ij) # Average over class sample count\n",
    "        conf_mat[i, j] = len(ind_ij)/sample_class_counts[j]\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(display_mat.astype(int))\n",
    "print(np.sum(display_mat))\n",
    "\n",
    "correct_class_samples = np.sum(np.diag(display_mat))\n",
    "print(\"Total Mumber of Misclassified Samples: {}\".format(N - correct_class_samples))\n",
    "\n",
    "prob_error = 1 - (correct_class_samples / N)\n",
    "print(\"Empirically Estimated Probability of Error: {:.4f}\".format(prob_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139850\n"
     ]
    }
   ],
   "source": [
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.groupby(['year']).mean().to_numpy() \n",
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# covariance = df.std()\n",
    "# means = df.mean()\n",
    "\n",
    "\n",
    "\n",
    "# X = (df-df.mean())/df.std()\n",
    "# # print(X)\n",
    "# indexer = df.index.values\n",
    "# # print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "# columns = df.columns\n",
    "\n",
    "# mu = []\n",
    "\n",
    "# labels = set()\n",
    "# for ax in df.index:\n",
    "#     labels.add(ax)\n",
    "# # print(labels)\n",
    "\n",
    "\n",
    "    \n",
    "# for index in indexer:\n",
    "#     mu.append([np.mean(X[feature][index]) for feature in df])\n",
    "# mu = np.array(mu)\n",
    "# n = mu.shape[1]\n",
    "# Sigma = []\n",
    "# # print(mu)\n",
    "# for index in indexer:\n",
    "#     Sigma.append(np.cov([X[feature][index] for feature in df]))\n",
    "# print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
